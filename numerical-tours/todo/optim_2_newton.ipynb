{"worksheets": [{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Newton Method", "=============", "", "*Important:* Please read the [installation page](http://gpeyre.github.io/numerical-tours/installation_python/) for details about how to install the toolboxes.", "$\\newcommand{\\dotp}[2]{\\langle #1, #2 \\rangle}$", "$\\newcommand{\\enscond}[2]{\\lbrace #1, #2 \\rbrace}$", "$\\newcommand{\\pd}[2]{ \\frac{ \\partial #1}{\\partial #2} }$", "$\\newcommand{\\umin}[1]{\\underset{#1}{\\min}\\;}$", "$\\newcommand{\\umax}[1]{\\underset{#1}{\\max}\\;}$", "$\\newcommand{\\umin}[1]{\\underset{#1}{\\min}\\;}$", "$\\newcommand{\\uargmin}[1]{\\underset{#1}{argmin}\\;}$", "$\\newcommand{\\norm}[1]{\\|#1\\|}$", "$\\newcommand{\\abs}[1]{\\left|#1\\right|}$", "$\\newcommand{\\choice}[1]{ \\left\\{  \\begin{array}{l} #1 \\end{array} \\right. }$", "$\\newcommand{\\pa}[1]{\\left(#1\\right)}$", "$\\newcommand{\\diag}[1]{{diag}\\left( #1 \\right)}$", "$\\newcommand{\\qandq}{\\quad\\text{and}\\quad}$", "$\\newcommand{\\qwhereq}{\\quad\\text{where}\\quad}$", "$\\newcommand{\\qifq}{ \\quad \\text{if} \\quad }$", "$\\newcommand{\\qarrq}{ \\quad \\Longrightarrow \\quad }$", "$\\newcommand{\\ZZ}{\\mathbb{Z}}$", "$\\newcommand{\\CC}{\\mathbb{C}}$", "$\\newcommand{\\RR}{\\mathbb{R}}$", "$\\newcommand{\\EE}{\\mathbb{E}}$", "$\\newcommand{\\Zz}{\\mathcal{Z}}$", "$\\newcommand{\\Ww}{\\mathcal{W}}$", "$\\newcommand{\\Vv}{\\mathcal{V}}$", "$\\newcommand{\\Nn}{\\mathcal{N}}$", "$\\newcommand{\\NN}{\\mathcal{N}}$", "$\\newcommand{\\Hh}{\\mathcal{H}}$", "$\\newcommand{\\Bb}{\\mathcal{B}}$", "$\\newcommand{\\Ee}{\\mathcal{E}}$", "$\\newcommand{\\Cc}{\\mathcal{C}}$", "$\\newcommand{\\Gg}{\\mathcal{G}}$", "$\\newcommand{\\Ss}{\\mathcal{S}}$", "$\\newcommand{\\Pp}{\\mathcal{P}}$", "$\\newcommand{\\Ff}{\\mathcal{F}}$", "$\\newcommand{\\Xx}{\\mathcal{X}}$", "$\\newcommand{\\Mm}{\\mathcal{M}}$", "$\\newcommand{\\Ii}{\\mathcal{I}}$", "$\\newcommand{\\Dd}{\\mathcal{D}}$", "$\\newcommand{\\Ll}{\\mathcal{L}}$", "$\\newcommand{\\Tt}{\\mathcal{T}}$", "$\\newcommand{\\si}{\\sigma}$", "$\\newcommand{\\al}{\\alpha}$", "$\\newcommand{\\la}{\\lambda}$", "$\\newcommand{\\ga}{\\gamma}$", "$\\newcommand{\\Ga}{\\Gamma}$", "$\\newcommand{\\La}{\\Lambda}$", "$\\newcommand{\\si}{\\sigma}$", "$\\newcommand{\\Si}{\\Sigma}$", "$\\newcommand{\\be}{\\beta}$", "$\\newcommand{\\de}{\\delta}$", "$\\newcommand{\\De}{\\Delta}$", "$\\newcommand{\\phi}{\\varphi}$", "$\\newcommand{\\th}{\\theta}$", "$\\newcommand{\\om}{\\omega}$", "$\\newcommand{\\Om}{\\Omega}$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This tour explores the use of the Newton method for the unconstrained", "optimization of a smooth function"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["from __future__ import division", "import nt_toolbox as nt", "from nt_solutions import optim_2_newton as solutions", "%matplotlib inline", "%load_ext autoreload", "%autoreload 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Newton Method for Unconstrained Problems", "----------------------------------------", "We test here Newton method for the minimization of a 2-D function.", "", "", "We define a highly anisotropic function,", "the Rosenbrock function", "", "$$ g(x) = (1-x_1)^2 + 100 (x_2-x_1^2)^2 $$"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["f = lambda x1, x2: (1-x1).^2 + 100*(x2-x1.^2).^2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The minimum of the function is reached at $x^\\star=(1,1)$ and", "$f(x^\\star)=0$.", "", "", "Evaluate the function on a regular grid."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["x1 = linspace(-2, 2, 150)", "x2 = linspace(-.5, 3, 150)", "[X2, X1] = meshgrid(x2, x1)", "F = f(X1, X2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["3-D display."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["clf; surf(x2, x1, F, perform_hist_eq(F, 'linear'))", "shading interp", "camlight", "axis tight", "colormap jet"]}, {"cell_type": "markdown", "metadata": {}, "source": ["2-D display (histogram equalization helps to better visualize", "the iso-contours)."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["imageplot(perform_hist_eq(F, 'linear'))", "colormap jet(256)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gradient descent methods, that only use first order (gradient)", "information about $f$ are not able to efficiently minimize this", "function because of its high anisotropy.", "", "", "Define the gradient of $f$", "$$ \\nabla g(x) = \\pa{ \\pd{g(x)}{x_1}, \\pd{g(x)}{x_2} } =", "      \\pa{ 2 (x_1-1) + 400 x_1 (x_1^2-x_2),", "      200 (x_2-x_1^2) } \\in \\RR^2. $$"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["gradf = lambda x1, x2: [2*(x1-1) + 400*x1.*(x1.^2-x2); 200*(x2-x1.^2)]", "Gradf = lambda x: gradf(x(1), x(2))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compute its Hessian", "$$", "      Hf(x) =", "      \\begin{pmatrix}", "          \\frac{\\partial^2 g(x)}{\\partial x_1^2} &", "          \\frac{\\partial^2 g(x)}{\\partial x_1 \\partial x_2} \\\\", "          \\frac{\\partial^2 g(x)}{\\partial x_1 \\partial x_2} &", "          \\frac{\\partial^2 g(x)}{\\partial x_2^2}", "      \\end{pmatrix}", "      =", "      \\begin{pmatrix}", "          2 + 400 (x_1^2-x_2) + 800 x_1^2 & -400 x_1 \\\\", "          -400 x_1 & 200", "      \\end{pmatrix} \\in \\RR^{2 \\times 2}", "$$"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["hessf = lambda x1, x2: [2 + 400*(x1.^2-x2) + 800*x1.^2, -400*x1; ...", "                -400*x1,  200]", "Hessf = lambda x: hessf(x(1), x(2))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The Newton descent method starting from some $x^{(0)} \\in \\RR^2$,", "$$ x^{(\\ell+1)} = x^{(\\ell)} - Hf( x^{(\\ell)} )^{-1} \\nabla f(x^{(\\ell)}). $$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Exercise 1__", "", "Implement the Newton algorithm.", "Display the evolution of $f(x^{(\\ell)})$ and $\\norm{x^{(\\ell)}-x^{(+\\infty)}}$", "during the iterations.", "isplay"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["solutions.exo1()"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["## Insert your code here."]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Exercise 2__", "", "Display the evolution of $x^{(\\ell)}$, from several starting points."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["solutions.exo2()"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["## Insert your code here."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gradient and Divergence of Images", "---------------------------------", "Local differential operators like gradient, divergence and laplacian are", "the building blocks for variational image processing.", "", "", "Load an image $g \\in \\RR^N$ of $N=n \\times n$ pixels."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["n = 256", "g = rescale(load_image('lena', n))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Display it."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["imageplot(g)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For continuous functions, the gradient reads", "$$ \\nabla g(x) = \\pa{ \\pd{g(x)}{x_1}, \\pd{g(x)}{x_2} } \\in \\RR^2. $$", "(note that here, the variable $x$ denotes the 2-D spacial position).", "", "", "We discretize this differential operator using first order finite", "differences.", "$$ (\\nabla g)_i = ( g_{i_1,i_2}-g_{i_1-1,i_2}, g_{i_1,i_2}-g_{i_1,i_2-1} ) \\in \\RR^2. $$", "Note that for simplity we use periodic boundary conditions.", "", "", "Compute its gradient, using finite differences."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["s = [n 1: n-1]", "grad = lambda f: cat(3, f-f(s, : ), f-f(: , s))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["One thus has $ \\nabla : \\RR^N \\mapsto \\RR^{N \\times 2}. $"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["v = grad(g)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["One can display each of its components."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["imageplot(v(: , : , 1), 'd/ dx', 1, 2, 1)", "imageplot(v(: , : , 2), 'd/ dy', 1, 2, 2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["One can also display it using a color image."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["imageplot(v)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["One can display its magnitude $\\norm{\\nabla g(x)}$, which is large near edges."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["imageplot(sqrt(sum3(v.^2, 3)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The divergence operator maps vector field to images.", "For continuous vector fields $v(x) \\in \\RR^2$, it is defined as", "$$ \\text{div}(v)(x) = \\pd{v_1(x)}{x_1} +  \\pd{v_2(x)}{x_2} \\in \\RR. $$", "(note that here, the variable $x$ denotes the 2-D spacial position).", "It is minus the adjoint of the gadient, i.e. $\\text{div} = - \\nabla^*$.", "", "", "It is discretized, for $v=(v^1,v^2)$ as", "$$ \\text{div}(v)_i = v^1_{i_1+1,i_2} - v^1_{i_1,i_2} + v^2_{i_1,i_2+1} - v^2_{i_1,i_2} . $$"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["t = [2: n 1]", "div = lambda v: v(t, : , 1)-v(: , : , 1) + v(: , t, 2)-v(: , : , 2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The Laplacian operatore is defined as $\\Delta=\\text{div} \\circ  \\nabla =", "-\\nabla^* \\circ \\nabla$. It is thus a negative symmetric operator."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["delta = lambda f: div(grad(f))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Display $\\Delta f_0$."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["imageplot(delta(g))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check that the relation $ \\norm{\\nabla f} = - \\dotp{\\Delta f}{f}.  $"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["dotp = lambda a, b: sum(a(: ).*b(: ))", "fprintf('Should be 0: %.3i\\n', dotp(grad(g), grad(g)) + dotp(delta(g), g))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Newton Method in Image Processing", "---------------------------------", "We consider now the problem of denoising an image $y \\in \\RR^N$ where", "$N = n \\times n$ is the number of pixels ($n$ being the number of rows/columns in the image).", "", "", "Add noise to the clean image, to simulate a noisy image $y$."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["sigma = .1", "y = g + randn(n)*sigma"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Display the noisy image $y$."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["imageplot(clamp(y))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Denoising is obtained by minimizing the following functional", "$$ \\umin{x \\in \\RR^d} g(x) = \\frac{1}{2} \\norm{ y-x}^2 + \\la J(x) $$", "where $J(x)$ is a smoothed total variation of the image.", "$$ J(x) = \\sum_i \\norm{ (G x)_i }_{\\epsilon} $$", "where $ (Gx)_i \\in \\RR^2 $ is an approximation of the gradient of $x$ at pixel", "$i$ and for $u \\in \\RR^2$, we use the following smoothing of the", "$L^2$ norm in $\\RR^2$", "$$ \\norm{u}_\\epsilon = \\sqrt{ \\epsilon^2 + \\norm{u}^2 }, $$", "for a small value of $\\epsilon>0$.", "", "", "Value for $\\lambda$."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["lambda = .3/ 5"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Value for $\\epsilon$."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["epsilon = 1e-3"]}, {"cell_type": "markdown", "metadata": {}, "source": ["TV norm."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["Amplitude = lambda u: sqrt(epsilon^2 + sum(u.^2, 3))", "J = lambda x: sum(sum(Amplitude(grad(x))))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Function to minimize."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["f = lambda x: 1/ 2*norm(x-y, 'fro')^2 + lambda*J(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The gradient of the functional read", "$$ \\nabla g(x) = x-y + \\lambda \\nabla J(x) $$", "where the gradient of the smoothed TV norm is", "$$ \\nabla J(x)_i = G^*( u ) \\qwhereq", "   u_i = \\frac{ (G x)_i }{\\norm{ (G x)_i }_\\epsilon} $$", "where $G^*$ is the adjoint operator of $G$ which corresponds to minus", "a discretized divergence.", "", "", "Define the gradient of TV norm. Note that |div| implement $-G^*$."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["Normalize = lambda u: u./ repmat(Amplitude(u), [1 1 2])", "GradJ = lambda x: -div(Normalize(grad(x)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gradient of the functional."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["Gradf = lambda x: x-y + lambda*GradJ(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The gradient of the functional can be written as", "$$ \\nabla g(x) = x - y + G^* \\circ \\delta_{A(x)} \\circ G (x) $$", "where $A(x) \\in \\RR^N$ is the norm of the gradient", "$$ A(x)_i = \\norm{(G u)_i} $$", "and where, for $x \\in \\RR^N$ and $ a \\in \\RR^{N} $,", "$$ \\de_{a}(u)_i = \\frac{u_i}{ \\sqrt{\\epsilon^2+a_i^2} }. $$"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["A = lambda x: sqrt(sum(grad(x).^2, 3))", "delta = lambda a, u: u./ sqrt(epsilon^2 + repmat(a.^2, [1 1 2]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This shows that the Hessian of $f$ is", "$$ Hf(x) = \\text{Id} + G^* \\circ \\delta_{d(x)} \\circ G + G^* \\circ \\Delta_{x} (G(x)) $$", "where $\\Delta_{x}(u) $ is the derivative of the mapping $ x \\mapsto", "\\delta_{d(x)}(u) $.", "", "", "Note that since $f$ is strictly convex, $Hf(x)$ is a positive", "definite matrix.", "", "", "We use a quasi-Newton scheme", "$$ x^{(\\ell+1)} = x^{(\\ell)} - H_\\ell^{-1} \\nabla f(x^{(\\ell)}), $$", "where $ H_\\ell $ is an approximation of the Hessian $H f(x^{(\\ell)})$.", "", "", "We define this approximation as", "$$ H_\\ell = \\tilde H f(x^{(\\ell)})", "      \\qwhereq", "      \\tilde Hf(x) = \\text{Id} + G^* \\circ \\delta_{x} \\circ G", "$$", "", "", "$H_\\ell$ is a symmetric positive", "definite matrix, that is bounded by bellow by the identity,", "which ensures the convergence of the method.", "", "", "", "Implement $\\tilde H f(x)$. Note that is parameterized by $a=A(x)$."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["H = lambda z, a: z - lambda*div(delta(a, grad(z)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Implement $\\tilde H f(x)^{-1}$ using a conjugate gradient.", "Note that it takes as argument a previous solution |u_prev|."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["flat = lambda x: x(: ); flatI = lambda x: reshape(x, [n, n])", "tol = eps; maxit = 40", "Hinv = lambda u, a, u_prev: cgs(lambda z: flat(H(flatI(z), a)), flat(u), tol, maxit, [], [], flat(u_prev))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialize the algorithm $x^{(0)} = y$."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["x = y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compute the vector $d = H_\\ell^{-1} \\nabla f(x^{(\\ell)})$.", "", "", "_Important:_ to get read of the warning message, you should use the code", "as this:"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["d = zeros(n); % replace thie line by the previous iterate d value", "[d, ~] = Hinv(Gradf(x), A(x), d)", "d = flatI(d)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compute $x^{(\\ell+1)} = x^{(\\ell)} - d$."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["x = x - d"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Exercise 3__", "", "Implement the Newton descent algorithm.", " d = Hinv(Gradf(x), A(x), d);"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["solutions.exo3()"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["## Insert your code here."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Display the result."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["imageplot(clamp(x))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Exercise 4__", "", "Compare the Newton descent with the gradient descent with a fixed step", "size, in term of decay of the energy."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["solutions.exo4()"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["## Insert your code here."]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Exercise 5__", "", "The direct comparison between gradient method and Newton is not fair in", "term of iteration count. Indeed, and iteration of Newton requires", "several steps of conjugate gradient, which takes some time.", "Try to set-up a fair comparison benchmark that takes into account the", "runing time of the methods. Pay a particular attention to the number of", "steps (or the tolerance criterion) that parameterize |cgs|."]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["solutions.exo5()"]}, {"cell_type": "code", "language": "python", "metadata": {}, "outputs": [], "collapsed": false, "input": ["## Insert your code here."]}]}], "nbformat": 3, "metadata": {"name": ""}, "nbformat_minor": 0}