def exo1():
    """
    Implement the coding of the vector |x| to obtain a binary vector |y|, which corresponds to replacing
    each sybmol |x(i)| by the code |C{x(i)}|.
    """
    y = []
    for i in 1: length(x):
        y = [y cell_get(C, x(i))]


def exo2():
    """
    For various values of block size |k|, Perform the hufman coding and compute the length of the code.
    Compare with the entropy lower bound.
    ntropy bound
    """
    e = -sum(log2(h).*h)
    disp(['Entropy = ' num2str(e) '.'])
    qlist = 1: 10
    err = []
    for q in qlist:
        % lifting
        n1 = ceil(n/ q)*q
        x1 = x
        x1(length(x1) + 1: n1) = 1
        x1 = reshape(x1, [q n1/ q])
        [Y, X] = meshgrid(1: n1/ q, 0: q-1)
        x1 = sum((x1-1) .* (m.^X), 1)' + 1
        % Probability table
        H = h
    for i in 1: q-1:
            H = kron(H, h)
        % compute the tree
        T = compute_hufftree(H)
        % do the coding
        y = perform_huffcoding(x1, T, + 1)
        % average number of bits
        e1 = length(y)/ length(x)
        err(q) = e1-e
        disp(['Huffman(block size ' num2str(q) ') = ' num2str(e1)])
    plot(qlist, err, '.-')
    set_label('q', 'entropy-code.length')
    axis('tight')


def exo3():
    """
    Compare the average number of bits per symbol generated by the arithmetic coder
    and the Shanon bound.
    omparison with entropy bound
    """
    nb = length(y)
    e1 = nb/ n; % number of bit per symbol
    e = -sum(log2(h).*h); % you have to use here the formula of the entropy
    disp(strcat(['Entropy = ' num2str(e, 3) ', arithmetic = ' num2str(e1, 3) '.']))


def exo4():
    """
    Encode a signal with an increasing size |n|, and check how close the
    generated signal coding rate |length(y)/n| becomes close to the optimal
    Shannon bound.
    ompute the differencial of coding for a varying length signal
    """
    e = -sum(h.*log2(h))
    err = []
    slist = 4: 12
    for i  in  1: length(slist):
        n = 2^slist(i)
        x = rand_discr(h, n)
        % coding
        y = perform_arith_fixed(x(: ), h)
        nb = length(y)
        e1 = nb/ n; % number of bits per symbol
        err(i) = e1 - e
    plot(slist, err, '.-'); axis('tight')
    set_label('log2(size)', '|entropy-nbr.bits|')


